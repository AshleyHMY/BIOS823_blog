{
  
    
        "post0": {
            "title": "823 final project",
            "content": "Project Github repository: https://github.com/JiamanBettyWu/bios823_project. Team:ABC . This is a group project where we used the MIMIC III to predict the outcome(death or not) after admitted to ICU. There are three main sections. The first is data cleaning, preprocessing, exploratory data analysis. Second, build a logistic regression model and random forest classification model to predict primary outcome. Third, build a multilayer perceptron to predict primary outcome. This report is about the second section of our project. To see more detailed information, please visit our girhub repository provided above. Our primary outcome is HOSPITAL_EXPIRE_FLAG. It is a bianry outcome that HOSPITAL_EXPIRE_FLAG=0 indicates patient survived in this hospitalization and HOSPITAL_EXPIRE_FLAG=1 indicates patient died in this hospitalization. . Build logistic regression model and random forest model to predict whether a patient dies after admitted into ICU. . by Ashley Mengyi Hu . import pandas as pd import numpy as np import random from sklearn import preprocessing from sklearn.model_selection import train_test_split from sklearn import metrics from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, precision_score, recall_score from sklearn.feature_selection import RFE from sklearn.linear_model import LassoCV, Lasso from sklearn.model_selection import cross_val_score, RandomizedSearchCV, RepeatedKFold from sklearn.ensemble import RandomForestClassifier from sklearn.preprocessing import LabelEncoder import matplotlib.pyplot as plt import statsmodels.api as sm import seaborn as sns %matplotlib inline from numpy import mean from numpy import std from pprint import pprint import math import plotly.express as px . df = pd.read_csv(&#39;MIMIC_cleaned.csv&#39;) . age = df.AGE_ON_AD.astype(str).str.split(&quot;.&quot;, expand=True)[[0]] age = age.astype(int) np.sort(age[0][age[0] &lt; 100].unique()) df[&quot;age&quot;] = np.where(age[0] &gt; 89, 90, age[0]) . le = LabelEncoder() df[&quot;gender&quot;] = le.fit_transform(df.GENDER) . df[&quot;ethnicity&quot;] = le.fit_transform(df.ETHNICITY) . Missing data . There are several missingness in patient DIAGNOSIS. We decided to replace missing data in diagnosis with 0. . df.fillna(0, inplace=True) . Distribution of age and ICU stay in days . We used plotly express to draw density plots of age and icu stays in days. . fig = px.histogram(df, x=&#39;age&#39;, color=&#39;HOSPITAL_EXPIRE_FLAG&#39;,facet_col=&#39;HOSPITAL_EXPIRE_FLAG&#39;, title = &#39;Distribution of age&#39;) fig.show() . Age distribution is similar between the two groups. In both groups as age increases the number of hospitalization increases. . fig = px.histogram(df, x=&#39;ICU_STAY_DAYS&#39;, color=&#39;HOSPITAL_EXPIRE_FLAG&#39;, facet_col=&#39;HOSPITAL_EXPIRE_FLAG&#39;, title = &#39;Distribution of ICU stay (days)&#39;) fig.show() . The two groups also have similar distribution in ICU stay. . train and test data split . y = df[&#39;HOSPITAL_EXPIRE_FLAG&#39;] . features_li = [&#39;age&#39;, &#39;gender&#39;, &#39;ethnicity&#39;,&#39;ICU_STAY_DAYS&#39;, &#39;MULTI_ENTRY_ICU&#39;, &#39;ICD9_9955&#39;,&#39;ICD9_3893&#39;, &#39;ICD9_9604&#39;, &#39;ICD9_966&#39;, &#39;ICD9_9672&#39;, &#39;ICD9_9904&#39;, &#39;ICD9_9671&#39;, &#39;ICD9_3961&#39;, &#39;ICD9_8856&#39;, &#39;ICD9_3891&#39;, &#39;PROP_ITEMID_51221&#39;, &#39;PROP_ITEMID_51222&#39;, &#39;PROP_ITEMID_51248&#39;, &#39;PROP_ITEMID_51249&#39;, &#39;PROP_ITEMID_51250&#39;, &#39;PROP_ITEMID_51265&#39;, &#39;PROP_ITEMID_51277&#39;, &#39;PROP_ITEMID_51279&#39;, &#39;PROP_ITEMID_51301&#39;, &#39;PROP_ITEMID_50820&#39;, &#39;PROP_ITEMID_50868&#39;, &#39;PROP_ITEMID_50882&#39;, &#39;PROP_ITEMID_50902&#39;, &#39;PROP_ITEMID_50912&#39;, &#39;PROP_ITEMID_50931&#39;, &#39;PROP_ITEMID_50960&#39;, &#39;PROP_ITEMID_50970&#39;, &#39;PROP_ITEMID_50971&#39;, &#39;PROP_ITEMID_50983&#39;, &#39;PROP_ITEMID_51006&#39;] X = df[features_li] . X.columns . Index([&#39;age&#39;, &#39;gender&#39;, &#39;ethnicity&#39;, &#39;ICU_STAY_DAYS&#39;, &#39;MULTI_ENTRY_ICU&#39;, &#39;ICD9_9955&#39;, &#39;ICD9_3893&#39;, &#39;ICD9_9604&#39;, &#39;ICD9_966&#39;, &#39;ICD9_9672&#39;, &#39;ICD9_9904&#39;, &#39;ICD9_9671&#39;, &#39;ICD9_3961&#39;, &#39;ICD9_8856&#39;, &#39;ICD9_3891&#39;, &#39;PROP_ITEMID_51221&#39;, &#39;PROP_ITEMID_51222&#39;, &#39;PROP_ITEMID_51248&#39;, &#39;PROP_ITEMID_51249&#39;, &#39;PROP_ITEMID_51250&#39;, &#39;PROP_ITEMID_51265&#39;, &#39;PROP_ITEMID_51277&#39;, &#39;PROP_ITEMID_51279&#39;, &#39;PROP_ITEMID_51301&#39;, &#39;PROP_ITEMID_50820&#39;, &#39;PROP_ITEMID_50868&#39;, &#39;PROP_ITEMID_50882&#39;, &#39;PROP_ITEMID_50902&#39;, &#39;PROP_ITEMID_50912&#39;, &#39;PROP_ITEMID_50931&#39;, &#39;PROP_ITEMID_50960&#39;, &#39;PROP_ITEMID_50970&#39;, &#39;PROP_ITEMID_50971&#39;, &#39;PROP_ITEMID_50983&#39;, &#39;PROP_ITEMID_51006&#39;], dtype=&#39;object&#39;) . X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0) . X_train.shape . (47180, 35) . X_test.shape . (11796, 35) . Stat model logistic regression . In this section we used sm.logit from the statmodel package to build logistic regression model. The stat model can provide coef estimates, p-value, thus we decided to use the statmodel package instead of using the logisticregression function from sklearn package. . X_train = np.array(X_train, dtype=float) X_test = np.array(X_test, dtype=float) . y_train = np.array(y_train, dtype=float) . logit = sm.Logit(y_train, X_train, solver=&#39;lbfgs&#39;).fit() . Optimization terminated successfully. Current function value: 0.250280 Iterations 12 . values = list() for i in range(35): value = (-1)*math.log(logit.pvalues[i]) values.append(value) . p_value = list(logit.pvalues) . coef = list(logit.params) . OR = list() for i in range(35): exp = math.exp(logit.params[i]) OR.append(exp) . features = [&#39;age&#39;, &#39;gender&#39;, &#39;ethnicity&#39;, &#39;ICU_STAY_DAYS&#39;, &#39;MULTI_ENTRY_ICU&#39;, &#39;ICD9_9955&#39;, &#39;ICD9_3893&#39;, &#39;ICD9_9604&#39;, &#39;ICD9_966&#39;, &#39;ICD9_9672&#39;, &#39;ICD9_9904&#39;, &#39;ICD9_9671&#39;, &#39;ICD9_3961&#39;, &#39;ICD9_8856&#39;, &#39;ICD9_3891&#39;, &#39;PROP_ITEMID_51221&#39;, &#39;PROP_ITEMID_51222&#39;, &#39;PROP_ITEMID_51248&#39;, &#39;PROP_ITEMID_51249&#39;, &#39;PROP_ITEMID_51250&#39;, &#39;PROP_ITEMID_51265&#39;, &#39;PROP_ITEMID_51277&#39;, &#39;PROP_ITEMID_51279&#39;, &#39;PROP_ITEMID_51301&#39;, &#39;PROP_ITEMID_50820&#39;, &#39;PROP_ITEMID_50868&#39;, &#39;PROP_ITEMID_50882&#39;, &#39;PROP_ITEMID_50902&#39;, &#39;PROP_ITEMID_50912&#39;, &#39;PROP_ITEMID_50931&#39;, &#39;PROP_ITEMID_50960&#39;, &#39;PROP_ITEMID_50970&#39;, &#39;PROP_ITEMID_50971&#39;, &#39;PROP_ITEMID_50983&#39;, &#39;PROP_ITEMID_51006&#39;] . logit.summary(xname=features) . Logit Regression Results Dep. Variable: y | No. Observations: 47180 | . Model: Logit | Df Residuals: 47145 | . Method: MLE | Df Model: 34 | . Date: Wed, 24 Nov 2021 | Pseudo R-squ.: 0.2233 | . Time: 20:00:52 | Log-Likelihood: -11808. | . converged: True | LL-Null: -15203. | . Covariance Type: nonrobust | LLR p-value: 0.000 | . | coef std err z P&gt;|z| [0.025 0.975] . age -0.0015 | 0.001 | -1.693 | 0.090 | -0.003 | 0.000 | . gender -0.3030 | 0.035 | -8.760 | 0.000 | -0.371 | -0.235 | . ethnicity -0.0526 | 0.001 | -37.020 | 0.000 | -0.055 | -0.050 | . ICU_STAY_DAYS -0.0330 | 0.003 | -12.024 | 0.000 | -0.038 | -0.028 | . MULTI_ENTRY_ICU -0.4389 | 0.044 | -10.028 | 0.000 | -0.525 | -0.353 | . ICD9_9955 -6.1602 | 0.710 | -8.680 | 0.000 | -7.551 | -4.769 | . ICD9_3893 -0.0314 | 0.033 | -0.960 | 0.337 | -0.096 | 0.033 | . ICD9_9604 0.0792 | 0.049 | 1.610 | 0.107 | -0.017 | 0.176 | . ICD9_966 -0.2444 | 0.051 | -4.780 | 0.000 | -0.345 | -0.144 | . ICD9_9672 1.6592 | 0.066 | 25.320 | 0.000 | 1.531 | 1.788 | . ICD9_9904 0.0831 | 0.050 | 1.670 | 0.095 | -0.014 | 0.181 | . ICD9_9671 1.0128 | 0.052 | 19.437 | 0.000 | 0.911 | 1.115 | . ICD9_3961 -1.2520 | 0.102 | -12.333 | 0.000 | -1.451 | -1.053 | . ICD9_8856 -0.1380 | 0.069 | -1.989 | 0.047 | -0.274 | -0.002 | . ICD9_3891 0.3440 | 0.049 | 7.025 | 0.000 | 0.248 | 0.440 | . PROP_ITEMID_51221 -0.6713 | 0.115 | -5.828 | 0.000 | -0.897 | -0.446 | . PROP_ITEMID_51222 -0.9473 | 0.107 | -8.860 | 0.000 | -1.157 | -0.738 | . PROP_ITEMID_51248 -0.2517 | 0.060 | -4.200 | 0.000 | -0.369 | -0.134 | . PROP_ITEMID_51249 -0.4173 | 0.063 | -6.649 | 0.000 | -0.540 | -0.294 | . PROP_ITEMID_51250 -0.2573 | 0.070 | -3.663 | 0.000 | -0.395 | -0.120 | . PROP_ITEMID_51265 0.7674 | 0.050 | 15.478 | 0.000 | 0.670 | 0.865 | . PROP_ITEMID_51277 0.1819 | 0.045 | 4.043 | 0.000 | 0.094 | 0.270 | . PROP_ITEMID_51279 -0.7037 | 0.091 | -7.766 | 0.000 | -0.881 | -0.526 | . PROP_ITEMID_51301 0.9536 | 0.049 | 19.513 | 0.000 | 0.858 | 1.049 | . PROP_ITEMID_50820 0.6368 | 0.049 | 12.916 | 0.000 | 0.540 | 0.733 | . PROP_ITEMID_50868 1.6488 | 0.101 | 16.273 | 0.000 | 1.450 | 1.847 | . PROP_ITEMID_50882 0.4671 | 0.059 | 7.889 | 0.000 | 0.351 | 0.583 | . PROP_ITEMID_50902 -0.1907 | 0.064 | -2.993 | 0.003 | -0.316 | -0.066 | . PROP_ITEMID_50912 0.2680 | 0.057 | 4.696 | 0.000 | 0.156 | 0.380 | . PROP_ITEMID_50931 -0.2988 | 0.062 | -4.802 | 0.000 | -0.421 | -0.177 | . PROP_ITEMID_50960 0.3002 | 0.093 | 3.233 | 0.001 | 0.118 | 0.482 | . PROP_ITEMID_50970 -0.0653 | 0.062 | -1.054 | 0.292 | -0.187 | 0.056 | . PROP_ITEMID_50971 0.1124 | 0.097 | 1.164 | 0.244 | -0.077 | 0.302 | . PROP_ITEMID_50983 1.4826 | 0.076 | 19.561 | 0.000 | 1.334 | 1.631 | . PROP_ITEMID_51006 0.8922 | 0.062 | 14.364 | 0.000 | 0.770 | 1.014 | . data_tuples = list(zip(features, coef, OR, p_value, values)) est = pd.DataFrame(data_tuples, columns=[&#39;features&#39;,&#39;coef&#39;, &#39;OR&#39;,&#39;p_value&#39;,&#39;-log(p_value)&#39;]) . est.sort_values(by=[&#39;OR&#39;], ascending=False) . features coef OR p_value -log(p_value) . 9 ICD9_9672 | 1.659162 | 5.254905 | 1.935743e-141 | 324.004007 | . 25 PROP_ITEMID_50868 | 1.648826 | 5.200871 | 1.522623e-59 | 135.432086 | . 33 PROP_ITEMID_50983 | 1.482553 | 4.404175 | 3.334329e-85 | 194.515461 | . 11 ICD9_9671 | 1.012778 | 2.753240 | 3.721535e-84 | 192.103011 | . 23 PROP_ITEMID_51301 | 0.953615 | 2.595075 | 8.566025e-85 | 193.571929 | . 34 PROP_ITEMID_51006 | 0.892219 | 2.440540 | 8.720228e-47 | 106.055854 | . 20 PROP_ITEMID_51265 | 0.767410 | 2.154180 | 4.907224e-54 | 122.748887 | . 24 PROP_ITEMID_50820 | 0.636820 | 1.890460 | 3.673814e-38 | 86.197003 | . 26 PROP_ITEMID_50882 | 0.467059 | 1.595296 | 3.055425e-15 | 33.421858 | . 14 ICD9_3891 | 0.344044 | 1.410641 | 2.136329e-12 | 26.871932 | . 30 PROP_ITEMID_50960 | 0.300181 | 1.350103 | 1.223747e-03 | 6.705838 | . 28 PROP_ITEMID_50912 | 0.268008 | 1.307357 | 2.649139e-06 | 12.841276 | . 21 PROP_ITEMID_51277 | 0.181919 | 1.199517 | 5.283794e-05 | 9.848281 | . 32 PROP_ITEMID_50971 | 0.112401 | 1.118962 | 2.443618e-01 | 1.409105 | . 10 ICD9_9904 | 0.083129 | 1.086682 | 9.501108e-02 | 2.353762 | . 7 ICD9_9604 | 0.079174 | 1.082392 | 1.073586e-01 | 2.231581 | . 0 age | -0.001469 | 0.998533 | 9.040222e-02 | 2.403486 | . 6 ICD9_3893 | -0.031397 | 0.969091 | 3.370979e-01 | 1.087382 | . 3 ICU_STAY_DAYS | -0.032966 | 0.967572 | 2.663404e-33 | 75.005703 | . 2 ethnicity | -0.052573 | 0.948785 | 5.498665e-300 | 689.071023 | . 31 PROP_ITEMID_50970 | -0.065338 | 0.936750 | 2.917913e-01 | 1.231716 | . 13 ICD9_8856 | -0.137952 | 0.871141 | 4.672626e-02 | 3.063449 | . 27 PROP_ITEMID_50902 | -0.190725 | 0.826359 | 2.758343e-03 | 5.893125 | . 8 ICD9_966 | -0.244410 | 0.783166 | 1.757158e-06 | 13.251813 | . 17 PROP_ITEMID_51248 | -0.251658 | 0.777511 | 2.664130e-05 | 10.533048 | . 19 PROP_ITEMID_51250 | -0.257294 | 0.773141 | 2.496878e-04 | 8.295299 | . 29 PROP_ITEMID_50931 | -0.298792 | 0.741714 | 1.568106e-06 | 13.365642 | . 1 gender | -0.302983 | 0.738611 | 1.955657e-18 | 40.775805 | . 18 PROP_ITEMID_51249 | -0.417257 | 0.658852 | 2.945470e-11 | 24.248167 | . 4 MULTI_ENTRY_ICU | -0.438919 | 0.644733 | 1.151114e-23 | 52.818727 | . 15 PROP_ITEMID_51221 | -0.671313 | 0.511037 | 5.604600e-09 | 18.999678 | . 22 PROP_ITEMID_51279 | -0.703667 | 0.494768 | 8.098853e-15 | 32.447054 | . 16 PROP_ITEMID_51222 | -0.947309 | 0.387783 | 8.006868e-19 | 41.668817 | . 12 ICD9_3961 | -1.252040 | 0.285921 | 5.994275e-35 | 78.799673 | . 5 ICD9_9955 | -6.160161 | 0.002112 | 3.968071e-18 | 40.068252 | . sm_pred = logit.predict(X_test).round() . array([0., 0., 1., ..., 0., 0., 0.]) . cnf_matrix = metrics.confusion_matrix(y_test, sm_pred.round()) . sns.set(rc={&#39;figure.figsize&#39;:(5,3.5)}) class_names=[0,1] # name of classes fig, ax = plt.subplots() tick_marks = np.arange(len(class_names)) plt.xticks(tick_marks, class_names) plt.yticks(tick_marks, class_names) # create heatmap sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=&quot;YlGnBu&quot; ,fmt=&#39;g&#39;) ax.xaxis.set_label_position(&quot;top&quot;) plt.tight_layout() plt.title(&#39;Confusion matrix&#39;, y=1.1) plt.ylabel(&#39;Actual label&#39;) plt.xlabel(&#39;Predicted label&#39;) . Text(0.5, 225.76, &#39;Predicted label&#39;) . print(&quot;Statmodel Logistic auc score:&quot;, roc_auc_score(y_test, sm_pred.round())) . Statmodel Logistic auc score: 0.6191858313582805 . print(&quot;Statmodel Logistic Accuracy:&quot;, accuracy_score(y_test, sm_pred.round())) print(&quot;Statmodel Logistic Precision:&quot;, precision_score(y_test, sm_pred.round())) print(&quot;Statmodel Logistic Recall:&quot;, recall_score(y_test, sm_pred.round())) print(classification_report(y_test, sm_pred.round())) . Statmodel Logistic Accuracy: 0.9042895896914208 Statmodel Logistic Precision: 0.5609318996415771 Statmodel Logistic Recall: 0.26148705096073516 precision recall f1-score support 0 0.92 0.98 0.95 10599 1 0.56 0.26 0.36 1197 accuracy 0.90 11796 macro avg 0.74 0.62 0.65 11796 weighted avg 0.88 0.90 0.89 11796 . Random forest hyperparameter tuning . First do a grid search to get the parameters that give the highest AUC score. We assigned three tree numbers and 4 max depth. . n_estimators = [int(x) for x in np.linspace(start = 600, stop = 1000, num = 3)] # Maximum number of levels in tree max_depth = [int(x) for x in np.linspace(100,500, num = 3)] max_depth.append(None) # Method of selecting samples for training each tree bootstrap = [True] class_weight = [&quot;balanced&quot;, &quot;balanced_subsample&quot;] # Create the random grid random_grid = {&#39;n_estimators&#39;: n_estimators, &#39;max_depth&#39;: max_depth, &#39;class_weight&#39;: class_weight} pprint(random_grid) . {&#39;class_weight&#39;: [&#39;balanced&#39;, &#39;balanced_subsample&#39;], &#39;max_depth&#39;: [100, 300, 500, None], &#39;n_estimators&#39;: [600, 800, 1000]} . # First create the base model to tune random.seed(2021) randomforest = RandomForestClassifier() rf_random = RandomizedSearchCV(estimator = randomforest, param_distributions = random_grid, n_iter = 1000, cv = 3, verbose=2, random_state=42, n_jobs = -1, scoring=&#39;roc_auc&#39;).fit(X_train, y_train) . Fitting 3 folds for each of 24 candidates, totalling 72 fits . /opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py:282: UserWarning: The total space of parameters 24 is smaller than n_iter=1000. Running 24 iterations. For exhaustive searches, use GridSearchCV. [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 33 tasks | elapsed: 6.0min /opt/conda/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak. [Parallel(n_jobs=-1)]: Done 72 out of 72 | elapsed: 14.0min finished . rf_random.best_params_ . {&#39;n_estimators&#39;: 1000, &#39;max_depth&#39;: None, &#39;class_weight&#39;: &#39;balanced&#39;} . y_pred = rf_random.best_estimator_.predict(X_test) . cnf_matrix = metrics.confusion_matrix(y_test, y_pred) . sns.set(rc={&#39;figure.figsize&#39;:(5,3.5)}) class_names=[0,1] # name of classes fig, ax = plt.subplots() tick_marks = np.arange(len(class_names)) plt.xticks(tick_marks, class_names) plt.yticks(tick_marks, class_names) # create heatmap sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=&quot;YlGnBu&quot; ,fmt=&#39;g&#39;) ax.xaxis.set_label_position(&quot;top&quot;) plt.tight_layout() plt.title(&#39;Confusion matrix&#39;, y=1.1) plt.ylabel(&#39;Actual label&#39;) plt.xlabel(&#39;Predicted label&#39;) . Text(0.5, 225.76, &#39;Predicted label&#39;) . print(&quot;Random forest auc score:&quot;, roc_auc_score(y_test, y_pred)) . Random forest auc score: 0.6271316795621471 . print(&quot;Random forest Accuracy:&quot;, accuracy_score(y_test, y_pred)) print(&quot;Random forest Precision:&quot;, precision_score(y_test, y_pred)) print(&quot;Random forest Recall:&quot;, recall_score(y_test, y_pred)) print(classification_report(y_test,y_pred)) . Random forest Accuracy: 0.9205662936588674 Random forest Precision: 0.8611111111111112 Random forest Recall: 0.25898078529657476 precision recall f1-score support 0 0.92 1.00 0.96 10599 1 0.86 0.26 0.40 1197 accuracy 0.92 11796 macro avg 0.89 0.63 0.68 11796 weighted avg 0.92 0.92 0.90 11796 . We built a random forest model with tree=1000 and balanced class weight. . feature_imp = pd.Series(rf_random.best_estimator_.feature_importances_,index=X.columns).sort_values(ascending=False) . sns.set(rc={&#39;figure.figsize&#39;:(10,12)}) sns.barplot(x=feature_imp, y=feature_imp.index) plt.xlabel(&#39;Feature Importance Score&#39;) plt.ylabel(&#39;Features&#39;) plt.title(&quot;Visualizing Important Features&quot;) plt.show() . This logistic regression model achived accuracy=0.9043, Precision=0.5609, Recall=0.2615, AUC score=0.6192. Based on the summary above, we identified 29 significant features that are associated with our primary outcome. The top five features that are mostly associated with death in ICU are Cont inv mec ven 96+ hrs （ICD9_9672), Anion Gap (Item 50868), Sodium (Item 50983), Cont inv mec ven &lt;96 hrs (ICD9_9671), 51301 White Blood Cells. . This random forest classification achieved accuracy=0.9206, Precision=0.8611, Recall=0.2590 and AUC score=0.6271. The top 5 most important features are Urea Nitrogen (labitem_51006), White Blood Cells (labitem_51301), age, Bicarbonate (labitem_50882), Icu stay in days. . The two models gave very different feature estimation. Only White Blood Cells (labitem_51301) was selected by both models as the top five important features. We found that the random forest has higher AUC score, higher precision and higher accuracy. In this project, the random forest has better performance on predicting whether a person would die after admitted to ICU. . Model performance . Here I included the scores of MLP as well. . Logistics regression Random Forest MLP . AUC | 0.62 | 0.63 | 0.71 | . Accuracy | 0.90 | 0.92 | 0.92 | . F1 | 0.36 | 0.40 | 0.55 | . Precision | 0.56 | 0.86 | 0.71 | . Recall | 0.26 | 0.26 | 0.44 | . MLP gives the highest AUC score, F1 score. Random forest has lower Recall. . Self-reflection: In this project I learned how modeling works in dealing with real life problem. My main responsibility for this project is to train a logistic regression model and a random forest model. In doing the random forest model, I used hyperparameter tuning to perform a grid search to find the proper parameter to build the random forest model. We also used the scoring = roc_auc to select the parameters that gives the highest auc score. My team members helped me a lot in building these two models. We had regular weekly meetings to discuss about our progress and make the process went smoothly. .",
            "url": "https://ashleyhmy.github.io/BIOS823_blog/2021/11/24/Final-project-self-section.html",
            "relUrl": "/2021/11/24/Final-project-self-section.html",
            "date": " • Nov 24, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Biostat823 Assigment5",
            "content": "Dive into Deep Learning | Train a deep learning model to classify beetles, cockroaches and dragonflies using these images. Note: Original images from https://www.insectimages.org/index.cfm. Blog about this, and explain how the neural network classified the images using SHapley Additive exPlanations. . by Mengyi Ashley Hu . import PIL from PIL import Image import glob import matplotlib.pyplot as plt import numpy as np import os import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers from tensorflow.keras.models import Sequential . path_train = &#39;train/&#39; train = [f for f in glob.glob(path_train + &quot;*/*.jpg&quot;, recursive=True)] . batch_size = 32 img_height = 180 img_width = 180 . train_ds = tf.keras.utils.image_dataset_from_directory( path_train, validation_split=0.2, subset=&quot;training&quot;, seed=123, image_size=(img_height, img_width), batch_size=batch_size) . Found 1019 files belonging to 3 classes. Using 816 files for training. . val_ds = tf.keras.utils.image_dataset_from_directory( path_train, validation_split=0.2, subset=&quot;validation&quot;, seed=123, image_size=(img_height, img_width), batch_size=batch_size) . Found 1019 files belonging to 3 classes. Using 203 files for validation. . class_names = train_ds.class_names print(class_names) . [&#39;beetles&#39;, &#39;cockroach&#39;, &#39;dragonflies&#39;] . plt.figure(figsize=(10, 10)) for images, labels in train_ds.take(1): for i in range(9): ax = plt.subplot(3, 3, i + 1) plt.imshow(images[i].numpy().astype(&quot;uint8&quot;)) plt.title(class_names[labels[i]]) plt.axis(&quot;off&quot;) . for image_batch, labels_batch in train_ds: print(image_batch.shape) print(labels_batch.shape) break . (32, 180, 180, 3) (32,) . AUTOTUNE = tf.data.AUTOTUNE train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE) val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE) . normalization_layer = layers.Rescaling(1./255) . normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y)) image_batch, labels_batch = next(iter(normalized_ds)) first_image = image_batch[0] # Notice the pixel values are now in `[0,1]`. print(np.min(first_image), np.max(first_image)) . 0.0 0.9841199 . data_augmentation = keras.Sequential( [ layers.RandomFlip(&quot;horizontal&quot;, input_shape=(img_height, img_width, 3)), layers.RandomRotation(0.1), layers.RandomZoom(0.1), ] ) . plt.figure(figsize=(10, 10)) for images, _ in train_ds.take(1): for i in range(9): augmented_images = data_augmentation(images) ax = plt.subplot(3, 3, i + 1) plt.imshow(augmented_images[0].numpy().astype(&quot;uint8&quot;)) plt.axis(&quot;off&quot;) . num_classes = 3 model = Sequential([data_augmentation, layers.Rescaling(1./255), layers.Conv2D(16, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;), layers.MaxPooling2D(), layers.Conv2D(32, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;), layers.MaxPooling2D(), layers.Conv2D(64, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;), layers.MaxPooling2D(), layers.Dropout(0.2), layers.Flatten(), layers.Dense(128, activation=&#39;relu&#39;), layers.Dense(num_classes) ]) . model.compile(optimizer=&#39;adam&#39;, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[&#39;accuracy&#39;]) . epochs=15 history = model.fit( train_ds, validation_data=val_ds, epochs=epochs ) . Epoch 1/15 26/26 [==============================] - 11s 405ms/step - loss: 0.9130 - accuracy: 0.6091 - val_loss: 0.6027 - val_accuracy: 0.8079 Epoch 2/15 26/26 [==============================] - 10s 394ms/step - loss: 0.6130 - accuracy: 0.7672 - val_loss: 0.4402 - val_accuracy: 0.8325 Epoch 3/15 26/26 [==============================] - 10s 399ms/step - loss: 0.5314 - accuracy: 0.7941 - val_loss: 0.3951 - val_accuracy: 0.8374 Epoch 4/15 26/26 [==============================] - 10s 402ms/step - loss: 0.4701 - accuracy: 0.8186 - val_loss: 0.3613 - val_accuracy: 0.8621 Epoch 5/15 26/26 [==============================] - 10s 387ms/step - loss: 0.4181 - accuracy: 0.8542 - val_loss: 0.4916 - val_accuracy: 0.7931 Epoch 6/15 26/26 [==============================] - 10s 386ms/step - loss: 0.4096 - accuracy: 0.8309 - val_loss: 0.3949 - val_accuracy: 0.8522 Epoch 7/15 26/26 [==============================] - 10s 389ms/step - loss: 0.3695 - accuracy: 0.8627 - val_loss: 0.3309 - val_accuracy: 0.8768 Epoch 8/15 26/26 [==============================] - 10s 386ms/step - loss: 0.3386 - accuracy: 0.8762 - val_loss: 0.4078 - val_accuracy: 0.8670 Epoch 9/15 26/26 [==============================] - 10s 389ms/step - loss: 0.3177 - accuracy: 0.8738 - val_loss: 0.3348 - val_accuracy: 0.9015 Epoch 10/15 26/26 [==============================] - 10s 392ms/step - loss: 0.2911 - accuracy: 0.8848 - val_loss: 0.3921 - val_accuracy: 0.8768 Epoch 11/15 26/26 [==============================] - 10s 388ms/step - loss: 0.2577 - accuracy: 0.8958 - val_loss: 0.3687 - val_accuracy: 0.8768 Epoch 12/15 26/26 [==============================] - 10s 392ms/step - loss: 0.2586 - accuracy: 0.9007 - val_loss: 0.4039 - val_accuracy: 0.8867 Epoch 13/15 26/26 [==============================] - 10s 401ms/step - loss: 0.2546 - accuracy: 0.9020 - val_loss: 0.3136 - val_accuracy: 0.8916 Epoch 14/15 26/26 [==============================] - 11s 405ms/step - loss: 0.2288 - accuracy: 0.9105 - val_loss: 0.4436 - val_accuracy: 0.8768 Epoch 15/15 26/26 [==============================] - 10s 391ms/step - loss: 0.2065 - accuracy: 0.9240 - val_loss: 0.3032 - val_accuracy: 0.9261 . model.summary() . Model: &#34;sequential_3&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= sequential_2 (Sequential) (None, 180, 180, 3) 0 rescaling_3 (Rescaling) (None, 180, 180, 3) 0 conv2d_6 (Conv2D) (None, 180, 180, 16) 448 max_pooling2d_6 (MaxPooling (None, 90, 90, 16) 0 2D) conv2d_7 (Conv2D) (None, 90, 90, 32) 4640 max_pooling2d_7 (MaxPooling (None, 45, 45, 32) 0 2D) conv2d_8 (Conv2D) (None, 45, 45, 64) 18496 max_pooling2d_8 (MaxPooling (None, 22, 22, 64) 0 2D) dropout_2 (Dropout) (None, 22, 22, 64) 0 flatten_2 (Flatten) (None, 30976) 0 dense_4 (Dense) (None, 128) 3965056 dense_5 (Dense) (None, 3) 387 ================================================================= Total params: 3,989,027 Trainable params: 3,989,027 Non-trainable params: 0 _________________________________________________________________ . visualization of training results by train dataset and validation dataset . acc = history.history[&#39;accuracy&#39;] val_acc = history.history[&#39;val_accuracy&#39;] loss = history.history[&#39;loss&#39;] val_loss = history.history[&#39;val_loss&#39;] epochs_range = range(epochs) plt.figure(figsize=(8, 8)) plt.subplot(1, 2, 1) plt.plot(epochs_range, acc, label=&#39;Training Accuracy&#39;) plt.plot(epochs_range, val_acc, label=&#39;Validation Accuracy&#39;) plt.legend(loc=&#39;lower right&#39;) plt.title(&#39;Training and Validation Accuracy&#39;) plt.subplot(1, 2, 2) plt.plot(epochs_range, loss, label=&#39;Training Loss&#39;) plt.plot(epochs_range, val_loss, label=&#39;Validation Loss&#39;) plt.legend(loc=&#39;upper right&#39;) plt.title(&#39;Training and Validation Loss&#39;) plt.show() . predict on new data . import matplotlib.pyplot as plt import matplotlib.image as mpimg . img = mpimg.imread(&#39;dragonfly.jpg&#39;) plt.imshow(img) . &lt;matplotlib.image.AxesImage at 0x7f63d349dd10&gt; . img = tf.keras.utils.load_img( &#39;dragonfly.jpg&#39;, target_size=(img_height, img_width) ) img_array = tf.keras.utils.img_to_array(img) img_array = tf.expand_dims(img_array, 0) # Create a batch predictions = model.predict(img_array) score = tf.nn.softmax(predictions[0]) print(np.argmax(score)) print( &quot;This image most likely belongs to {} with a {:.2f} percent confidence.&quot; .format(class_names[np.argmax(score)], 100 * np.max(score))) . 2 This image most likely belongs to dragonflies with a 100.00 percent confidence. . Use the test set to calculate accuracy . path_test = &#39;test/&#39; test = [f for f in glob.glob(path_test + &quot;*/*.jpg&quot;, recursive=True)] . test_ds = tf.keras.utils.image_dataset_from_directory( path_test, image_size=(img_height, img_width), batch_size=batch_size) . Found 180 files belonging to 3 classes. . predictions = model.predict(test_ds) len(predictions) . 180 . score = tf.nn.softmax(predictions) type(score) . tensorflow.python.framework.ops.EagerTensor . pred_test_label = [] for i in range(180): score = tf.nn.softmax(predictions[i]) class_img = np.argmax(score) pred_test_label.append(class_img) . test_label = np.concatenate([y for x, y in test_ds], axis = 0) . acc = np.sum(np.equal(test_label,pred_test_label))/len(test_label) print(&quot;The accuracy calculated using test data is {}&quot;.format(acc)) . The accuracy calculated using test data is 0.3055555555555556 . . Shapley expanation . import shap . x_train = np.concatenate([x for x, y in train_ds], axis = 0) y_train = np.concatenate([y for x, y in train_ds], axis = 0) . x_test = np.concatenate([x for x, y in test_ds], axis = 0) y_test = np.concatenate([y for x, y in test_ds], axis = 0) . explainer = shap.GradientExplainer(model, x_train) . sv = explainer.shap_values(x_test[:20]) . shap.image_plot([sv[i] for i in range(3)], x_test[:3]) .",
            "url": "https://ashleyhmy.github.io/BIOS823_blog/2021/11/15/823-Assignment5.html",
            "relUrl": "/2021/11/15/823-Assignment5.html",
            "date": " • Nov 15, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Biostat823 Assignment3",
            "content": "Biostat823 assignent3 Ashley Hu . 3. Creating effective visualizations using best practices . Create 3 informative visualizations about malaria using Python in a Jupyter notebook, starting with the data sets at https://github.com/rfordatascience/tidytuesday/tree/master/data/2018/2018-11-13. Where appropriate, make the visualizations interactive. . Note There are many libraries you can use for each task. Choose one library and explain why you chose it in your blog. . This assignment use three dataset namely malaria_deaths.csv, malaria_deaths_age.csv, malaria_inc.scv that give information on death rate for all age group(between 1990 and 2016), the number of death cases grouped by age (between 1990 and 2016) and the incidence of malaria (between 2000 and 2015). I used the plotting library bokeh to generate plot1 and plot2. I used the plotting library matplotlib to generate plot3. . import csv import numpy as np import pandas as pd . pip install pandas-bokeh . Requirement already satisfied: pandas-bokeh in /opt/conda/lib/python3.7/site-packages (0.5.5) Requirement already satisfied: pandas&gt;=0.22.0 in /opt/conda/lib/python3.7/site-packages (from pandas-bokeh) (1.1.0) Requirement already satisfied: bokeh&gt;=2.0 in /opt/conda/lib/python3.7/site-packages (from pandas-bokeh) (2.1.1) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas&gt;=0.22.0-&gt;pandas-bokeh) (2.8.1) Requirement already satisfied: pytz&gt;=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas&gt;=0.22.0-&gt;pandas-bokeh) (2020.1) Requirement already satisfied: numpy&gt;=1.15.4 in /opt/conda/lib/python3.7/site-packages (from pandas&gt;=0.22.0-&gt;pandas-bokeh) (1.19.1) Requirement already satisfied: typing-extensions&gt;=3.7.4 in /opt/conda/lib/python3.7/site-packages (from bokeh&gt;=2.0-&gt;pandas-bokeh) (3.7.4.2) Requirement already satisfied: PyYAML&gt;=3.10 in /opt/conda/lib/python3.7/site-packages (from bokeh&gt;=2.0-&gt;pandas-bokeh) (5.3.1) Requirement already satisfied: packaging&gt;=16.8 in /opt/conda/lib/python3.7/site-packages (from bokeh&gt;=2.0-&gt;pandas-bokeh) (20.4) Requirement already satisfied: Jinja2&gt;=2.7 in /opt/conda/lib/python3.7/site-packages (from bokeh&gt;=2.0-&gt;pandas-bokeh) (2.11.2) Requirement already satisfied: pillow&gt;=4.0 in /opt/conda/lib/python3.7/site-packages (from bokeh&gt;=2.0-&gt;pandas-bokeh) (7.2.0) Requirement already satisfied: tornado&gt;=5.1 in /opt/conda/lib/python3.7/site-packages (from bokeh&gt;=2.0-&gt;pandas-bokeh) (6.0.4) Requirement already satisfied: six&gt;=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas&gt;=0.22.0-&gt;pandas-bokeh) (1.15.0) Requirement already satisfied: pyparsing&gt;=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging&gt;=16.8-&gt;bokeh&gt;=2.0-&gt;pandas-bokeh) (2.4.7) Requirement already satisfied: MarkupSafe&gt;=0.23 in /opt/conda/lib/python3.7/site-packages (from Jinja2&gt;=2.7-&gt;bokeh&gt;=2.0-&gt;pandas-bokeh) (1.1.1) Note: you may need to restart the kernel to use updated packages. . import pandas_bokeh from bokeh.io import output_notebook, curdoc from bokeh.plotting import figure, show, output_notebook, output_file from bokeh.layouts import gridplot, row, column from bokeh.models import ColumnDataSource, DataRange1d from bokeh.models.tools import HoverTool from bokeh.transform import factor_cmap from bokeh.palettes import brewer from bokeh.tile_providers import CARTODBPOSITRON, get_provider import ipywidgets as widgets output_notebook() . Loading BokehJS ... death = pd.read_csv(&#39;malaria_deaths.csv&#39;) death = death.rename(columns={&quot;Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)&quot;:&quot;death_rate&quot;}) # To print the first 5 rows of the malaria_deaths.csv file death.head() . Entity Code Year death_rate . 0 Afghanistan | AFG | 1990 | 6.802930 | . 1 Afghanistan | AFG | 1991 | 6.973494 | . 2 Afghanistan | AFG | 1992 | 6.989882 | . 3 Afghanistan | AFG | 1993 | 7.088983 | . 4 Afghanistan | AFG | 1994 | 7.392472 | . deathage = pd.read_csv(&#39;malaria_deaths_age.csv&#39;) deathage = deathage.rename(columns = {&#39;age_group&#39;:&#39;agegroup&#39;}) #To print the first 5 rows of the malaria_deaths_age.csv deathage.head() . Unnamed: 0 entity code year agegroup deaths . 0 1 | Afghanistan | AFG | 1990 | Under 5 | 184.606435 | . 1 2 | Afghanistan | AFG | 1991 | Under 5 | 191.658193 | . 2 3 | Afghanistan | AFG | 1992 | Under 5 | 197.140197 | . 3 4 | Afghanistan | AFG | 1993 | Under 5 | 207.357753 | . 4 5 | Afghanistan | AFG | 1994 | Under 5 | 226.209363 | . inc = pd.read_csv(&#39;malaria_inc.csv&#39;) inc = inc.rename(columns = {&quot;Incidence of malaria (per 1,000 population at risk) (per 1,000 population at risk)&quot;:&quot;incidence&quot;}) #To print the first 5 rows of the malaria_inc.csv inc.head() . Entity Code Year incidence . 0 Afghanistan | AFG | 2000 | 107.100000 | . 1 Afghanistan | AFG | 2005 | 46.500000 | . 2 Afghanistan | AFG | 2010 | 23.900000 | . 3 Afghanistan | AFG | 2015 | 23.600000 | . 4 Algeria | DZA | 2000 | 0.037746 | . Plot1 . I use bokeh to draw two interactive line plots to represent the incidence and overall death rate of malaria. The bokeh library offers hovertools to show the data at each datapoint by pointing at each dot. It also offer a tool bar where I can zoom-in and zoom-out the plot. . def get_incidence_plot(Country): &quot;&quot;&quot; Country is the input, the input is the name of a country. eg. get_incidence_plot(&#39;China&#39;) &gt;&gt;&gt; The output will be a column of two plots. &quot;&quot;&quot; output_file(&quot;line_incidence.html&quot;) tooltips1 = [ (&#39;Incidence&#39;, &#39;@incidence&#39;), (&#39;Year&#39;, &#39;@Year&#39;) ] df1 = inc.loc[inc[&#39;Entity&#39;]==Country] source1 = ColumnDataSource(df1) p1 = figure(title = &#39;Incidence of Malaria (per 1,000 people at risk)&#39;, width = 600, height = 400) p1.line(x = &#39;Year&#39;, y = &#39;incidence&#39;, source = source1) p1.scatter(x = &#39;Year&#39;, y = &#39;incidence&#39;, source = source1) p1.xaxis.axis_label = &#39;Year&#39; p1.yaxis.axis_label = &#39;Incidence (per 1,000 people at risk)&#39; p1.add_tools(HoverTool(tooltips = tooltips1)) tooltips2 = [ (&#39;Death rate&#39;, &#39;@death_rate&#39;), (&#39;Year&#39;, &#39;@Year&#39;) ] df2 = death.loc[death[&#39;Entity&#39;]==Country] source2 = ColumnDataSource(df2) p2 = figure(title = &#39;Death rate of Malaria all age group (per 100,000 people)&#39;, width = 600, height = 400) p2.line(x = &#39;Year&#39;, y = &#39;death_rate&#39;, source = source2) p2.scatter(x = &#39;Year&#39;, y = &#39;death_rate&#39;, source = source2) p2.xaxis.axis_label = &#39;Year&#39; p2.yaxis.axis_label = &#39;Death rate (per 100,000 people)&#39; p2.add_tools(HoverTool(tooltips = tooltips2)) return show(column(p1,p2)) . widgets.interact(get_incidence_plot, Country = inc[&quot;Entity&quot;].unique()) . &lt;function __main__.get_incidence_plot(Country)&gt; . Plot 2 . I used the bokeh library to plot a stacked line plot to represent the number of deaths by year grouped by age. . def get_age_death_plot(Country): #Get data df1 = deathage.loc[(deathage[&#39;entity&#39;]==Country)] df2 = df1.pivot(index = &#39;year&#39;, columns = &#39;agegroup&#39;, values = &#39;deaths&#39;) data = dict( years = list(deathage[&#39;year&#39;].unique()), g1 = list(df2[&#39;Under 5&#39;]), g2 = list(df2[&#39;14-May&#39;]), g3 = list(df2[&#39;15-49&#39;]), g4 = list(df2[&#39;50-69&#39;]), g5 = list(df2[&#39;70 or older&#39;])) source = ColumnDataSource(data) #Draw the plot output_file(&quot;area_agedeath_grouped.html&quot;) p = figure(width = 700, height = 400, title = &#39;Death cases by age group&#39;, x_axis_label = &#39;Year&#39;, y_axis_label = &#39;Death case&#39;) p.varea_stack([&#39;g1&#39;, &#39;g2&#39;, &#39;g3&#39;, &#39;g4&#39;, &#39;g5&#39;], x=&#39;years&#39;, color = (&#39;lightsalmon&#39;, &#39;salmon&#39;,&#39;indianred&#39;,&#39;crimson&#39;,&#39;firebrick&#39;), legend_label = (&#39;&lt;5&#39;, &#39;6-14&#39;, &#39;15-49&#39;, &#39;50-69&#39;, &#39;&gt;70&#39;), source = source, fill_alpha = 0.9) p.legend.title = &#39;Age&#39; p.legend.location = &#39;top_left&#39; return show(p) . widgets.interact(get_age_death_plot, Country = deathage[&quot;entity&quot;].unique()) . &lt;function __main__.get_age_death_plot(Country)&gt; . Plot3 . I used the matplotlib library to generate a map to show the incidence of malaria between 2000 and 2015. I used the naturalearth shapefile to plot the country shape and use the incidence data to fill in the plot. . import geopandas as gpd import matplotlib.pyplot as plt . world = gpd.read_file(gpd.datasets.get_path(&#39;naturalearth_lowres&#39;)) . def get_incidence_geographical_plot(year): df = inc.loc[inc[&#39;Year&#39;]==year] key = df[&#39;Entity&#39;] gdf = world.merge(df, left_on = world[&#39;name&#39;], right_on = df[&#39;Entity&#39;], how = &#39;outer&#39;) fig, ax = plt.subplots(1, figsize=(12,5)) gdf.plot(column=&#39;incidence&#39;, cmap=&#39;OrRd&#39;, ax=ax, legend=True, legend_kwds={&#39;label&#39;: &quot;Incidence of malaria (per 1,000 population at risk)&quot;, &#39;orientation&#39;: &quot;horizontal&quot;}) . widgets.interact(get_incidence_geographical_plot, year = inc[&#39;Year&#39;].unique()) . &lt;function __main__.get_incidence_geographical_plot(year)&gt; .",
            "url": "https://ashleyhmy.github.io/BIOS823_blog/2021/10/01/Assignment3.html",
            "relUrl": "/2021/10/01/Assignment3.html",
            "date": " • Oct 1, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Biostat823 Assignment2",
            "content": "Number theory and a Google recruitment puzzle . Find the first 10-digit prime in the decimal expansion of 17π. . The first 5 digits in the decimal expansion of π are 14159. The first 4-digit prime in the decimal expansion of π are 4159. You are asked to find the first 10-digit prime in the decimal expansion of 17π. First solve sub-problems (divide and conquer): . Write a function to generate an arbitrary large expansion of a mathematical expression like π. Hint: You can use the standard library decimal or the 3rd party library sympy to do this | Write a function to check if a number is prime. Hint: See Sieve of Eratosthenes | Write a function to generate sliding windows of a specified width from a long iterable (e.g. a string representation of a number) | . Write unit tests for each of these three functions. You are encouraged, but not required, to try test-driven development. . Now use these helper functions to write the function that you need. Write a unit test for this final function, given that the first 10-digit prime in the expansion e is 7427466391. Finally, solve the given problem. . This assignment can be found in my github blog (named as Biostat 823 Assignment2): https://ashleyhmy.github.io/BIOS823_blog/ . import sympy as sym import math import unittest . def num_expansion(expr, args): &quot;&quot;&quot; generate an arbitary large expansion for a scientific expression like pi and e, returns numeric expression. expr is the mathematical expression to be converted eg, expr = sym.exp(1) use the sympy package to get scientific expression for e args is the number of significant numbers required eg, args = 5 Examples to get arbitary expression for e with 5 significant numbers: &gt;&gt;&gt; num_expansion(expr, 5) 2.7183 &quot;&quot;&quot; num = expr.evalf(args) return num . def is_prime(num): &quot;&quot;&quot; Take input num to check if num is a prime, if num is a prime return True, if num is not a prime return False. Example: &gt;&gt;&gt;is_prime(17) False &quot;&quot;&quot; if num&lt;2: return False if num==2: return True if num&gt;2 and num%2 == 0: return False for i in range(3, 1 + math.floor(math.sqrt(num)), 2): if num%i == 0: return False return True . def get_window(seq, digits): &quot;&quot;&quot; seq is the input, a list of numbers win_size is the size of the window Example: seq = [1,2,3,4,5,6] win_size = 2 &gt;&gt;&gt; window(seq, win_size, step) [[1,2], [2,3], [3,4], [4,5], [5,6]] &quot;&quot;&quot; num_of_chunk = int(len(seq)-digits + 1) for i in range(0, num_of_chunk): yield seq[i:i+digits] . class TestFunctions(unittest.TestCase): def test_num_expansion(self): result = pi.evalf(5) self.assertEqual(num_expansion(pi, 5), result) def test_is_prime(self): self.assertEqual(is_prime(17), True) self.assertEqual(is_prime(10), False) def test_window(self): seq = [1,2,3,4] self.assertEqual(list(get_window(seq, 3)), [[1,2,3],[2,3,4]]) if __name__ == &quot;__main__&quot;: unittest.main(argv=[&#39;&#39;], verbosity =2, exit=False) . test_is_prime (__main__.TestFunctions) ... ok test_num_expansion (__main__.TestFunctions) ... ok test_window (__main__.TestFunctions) ... ok - Ran 3 tests in 0.002s OK . def get_first_prime(expr, args, digits): &quot;&quot;&quot; The input expr is the methametical expression that want to be expanded The input args represents how many significant number is required from the methametical expression The input digits represents the number of digits is in the prime Example: &gt;&gt;&gt;get_first_prime(pi, 200, 10) 5926535897 &quot;&quot;&quot; num1 = num_expansion(expr, args) str1 = str(num1) list1 = str1.split(&#39;.&#39;) lst = [&#39;&#39;.join(list1[0:2])] num = lst[0] seq = [int(a) for a in str(num)] windows = list(window(seq, digits)) prime_lst = [] for win in windows: str1 = &#39;&#39;.join(map(str, win)) num_to_check = int(str1) if is_prime(num_to_check) == True: prime_lst.append(num_to_check) prime = prime_lst[0] return prime . class TestFunctions(unittest.TestCase): &#39;&#39;&#39;To check whether the output of get_first_prime() function is equal to 7427466391 &#39;&#39;&#39; def test_get_first_prime(self): expr = sym.exp(1) self.assertEqual(get_first_prime(expr, 200, 10), 7427466391) if __name__ == &quot;__main__&quot;: unittest.main(argv=[&#39;&#39;], verbosity =2, exit=False) . test_get_first_prime (__main__.TestFunctions) ... ok - Ran 1 test in 0.025s OK . get_first_prime(pi*17, 50, 10) print(&quot;The first 10 digit prime of 17 u03C0 is&quot;, get_first_prime(pi*17, 50, 10)) . The first 10 digit prime of 17π is 8649375157 .",
            "url": "https://ashleyhmy.github.io/BIOS823_blog/2021/09/17/823-Assignment2.html",
            "relUrl": "/2021/09/17/823-Assignment2.html",
            "date": " • Sep 17, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Biostat823 Assignment1",
            "content": "Biostat 823 assignment1-math is fun Ashley Hu . This assignment can also be found in my blog named Biostat 823 Assignment1. . The link for github repo is :https://github.com/AshleyHMY/BIOS823_blog . Q1 By listing the first six prime numbers 2,3,5,7,11 and 13, we can see that the 6th prime is 13. What is the 10001st prime number? (answered by 424,641 people) . n = 10001 prime_numbers = [2,3] i = 3 if (0&lt;n&lt;3): print(n, &#39;th prime number is:&#39;, prime_numbers[n-1]) elif(n&gt;2): while(True): i+=1 flag = False for j in range(2, int(i/2)+1): if (i%j==0): # i is not a prime number if i%j==0. Only need to check the first half of i. flag = True break if (flag==False): prime_numbers.append(i) if(len(prime_numbers)==n): break print(n, &#39;th prime number is:&#39;, prime_numbers[n-1]) else: print(&#39;Please enter a valid number&#39;) . 10001 th prime number is: 104743 . Explanation for Q1: In this question I want to find the 10001st prime number let n = 10001. I created a list called prime_numbers that includes the first two prime number 2 and 3. In the first if statement when n=1 the first prime number is 2 and n=2 the second prime number is 3. I added more elements(prime numbers) to the prime_numbers list. Then I want to check if integers greater than 3 is a prime. First let i=4 and I want to check if 4 is a prime using the for loop (for j in range(2, 3). Here I used int(i/2)+1 to reduce calculation (eg, 10=2x5, if 10 can be divded by 2, we can conclude 10 is not a prime, do not need to check if 10 can be divided by 5 again). If 4 can be divided by 2 or 3, flag will be equal to True and 4 will not be added to the prime_numbers list. Then let i=5 and run the for loop (for j in range(2, 3)). 5 cannot be divided by 2 or 3, thus, 5 is a prime number and 5 is the third element in the prime_numbers list. The code will print &quot;3th prime number is 5. let n=10001, the code will give the 10001th prime number. . Q2, The prime 41, can be written as the sum of six consecutive primes: 41=2+3+5+7+11+13 This is the longest sum of consecutive primes that adds to a prime below one-hundred. The longest sum of consecutive primes below one-thousand that adds to aprime, contains 21 terms, and is equal to 953. Which prime below one million can be written as the sum of the most consecutive primes?(Solved by 62,918 people) . import sympy sum=0 num_lst = [] sum_lst = [] prime_sum_lst = [] limit = 1000000 for num in range(1,limit): if sympy.isprime(num) is True: sum+=num num_lst.append(num) sum_lst.append(sum) for sum in sum_lst: if sum&lt;limit: if sympy.isprime(sum)==True: prime_sum_lst.append(sum) max_sum = max(prime_sum_lst) index = sum_lst.index(max_sum)+1 print(&quot;The required answer is :&quot;, max_sum, &#39;contains&#39;, index, &#39;items&#39;) . The required answer is : 958577 contains 536 items . Explanation for Q2: I use the function isprime from numpy. First, I created a list called num_lst that contains all elements below 1,000,000. I select all prime numbers from 1 to 1,000,000. The sum_lst list contains all possible consecutive sum(eg, 2,5,10,17,28,41). The second for loop select all possible sum betlow 1,000,000 that is a prime number. The prime_sum_lst includes elements (eg, 2,5,17,41). Use the mac() function to select the largest element in prime_sum_lst list. Then, use the index function to get position of max_sum in the sum_lst that is equal to the number of elements in this max_sum. . Q3, The smallest number expressible as the sum of a prime square, prime cube, and prime fourth power is 28. In fact, there are exactly four numbers below fifty that can be expressed in such a way: 28 = 2^2 + 2^3 + 2^4 33 = 3^2 + 2^3 + 2^4 How many numbers below fifty million can be expressed as the sum of a prime square, prime cube, and prime fourth power? (solved by 20805 people) . import numpy as np limit = 50000000 max_prime = int(limit**0.5) #All possible prime numbers should be smaller than the square root of fifty million. prime_sum = [] #To get a list of prime numbers below max_prime using for loop for num in range(2, max_prime): if sympy.isprime(num) is True: num_prime.append(num) for prime3 in num_prime: prime_forth_power = prime3**4 if prime_forth_power &gt; limit: break for prime2 in num_prime: prime_third_forth = prime2**3 + prime_forth_power if prime_third_forth &gt; limit: break for prime1 in num_prime: sum_power = prime1**2 + prime_third_forth if sum_power &gt; limit: break prime_sum.append(sum_power) def unique(list): &quot;&quot;&quot;This unique function selects unique elements in the prime_sum list generated from the above for loop&quot;&quot;&quot; x = np.array(list) unique_lst = np.unique(x) return unique_lst result = len(unique(prime_sum)) result . 1097343 . Explanation for Q3: First use an expression num = a^2+b^2+c^2. a should be smaller than the square root of fifty million. If a^2 is greater than fifty million, this number should not be included in the expression. The max_prime is the maximum number that can be included in the expression. The num_prime includes all primes that can be used in the expression. The for loop selects numbers consists of a prime square, a prime cube and a prime fourth for prime numbers between 2 and max_prime. There are duplicate numbers in the prime_sum list. I create a function called unique() to select unique values in the prime_sum list. The &quot;result&quot; represents the number of unique elements int eh prime_sum list. &quot;result&quot; equals to the number of numbers below fifty million that can be expressed as the sum of a prime square, prime cube and prime fourth power. .",
            "url": "https://ashleyhmy.github.io/BIOS823_blog/2021/09/03/823-Assignment1.html",
            "relUrl": "/2021/09/03/823-Assignment1.html",
            "date": " • Sep 3, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ashleyhmy.github.io/BIOS823_blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ashleyhmy.github.io/BIOS823_blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ashleyhmy.github.io/BIOS823_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ashleyhmy.github.io/BIOS823_blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}